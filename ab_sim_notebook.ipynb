{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.spatial import cKDTree\n",
    "import scipy.io\n",
    "import os\n",
    "import itertools\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Setting device. Should be fine to use as is \"\"\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == 'cuda':\n",
    "    print('Using cuda')\n",
    "    float_tensor = torch.cuda.FloatTensor\n",
    "else:\n",
    "    float_tensor = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_random_cube(N, non_polar_frac, size=20):\n",
    "\n",
    "    \"\"\" Returns a numpy array with random positions, APB and PCP\n",
    "        Along with a mask dictating which particles are non polarized  \"\"\"\n",
    "    \n",
    "    x = np.random.uniform(-size/2,size/2, size=(N,3))                                  #Particles positions\n",
    "\n",
    "    p = np.random.uniform(-1,1,size=(N,3))                                             #AB polarity unit vectors\n",
    "    p /= np.sqrt(np.sum(p**2, axis=1))[:,None]\n",
    "\n",
    "    q = np.random.uniform(-1,1,size=(N,3))                                             #PCP unit vectors\n",
    "    q /= np.sqrt(np.sum(p**2, axis=1))[:,None]\n",
    "\n",
    "    mask = np.random.choice([0,1], p=[non_polar_frac, 1-non_polar_frac], size=N)       #Mask detailing which particles are non polar\n",
    "    p[mask == 0] = np.array([0,0,0])                                                   #Setting the polarities of the non-polarized particles to 0\n",
    "    q[mask == 0] = np.array([0,0,0])\n",
    "\n",
    "    cube_data = np.concatenate((mask[:,None], x, p, q) ,axis=1)                        #Total data\n",
    "    return cube_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('cube1000_50p_nonppnonp',make_random_cube(1000, .5, size=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(str):\n",
    "    \n",
    "    #Importing data \n",
    "    data = np.load(str)\n",
    "    p_mask = data[:,0]\n",
    "    x = data[:,1:4]\n",
    "    p = data[:,4:7]\n",
    "    q = data[:,7:10]\n",
    "\n",
    "    #Casting all the data to the right torch tensors\n",
    "    x = torch.tensor(x, requires_grad=True, dtype=torch.float, device=device)\n",
    "    p = torch.tensor(p, requires_grad=True, dtype=torch.float, device=device)\n",
    "    q = torch.tensor(q, requires_grad=True, dtype=torch.float, device=device)\n",
    "    p_mask = torch.tensor(p_mask, dtype=torch.int, device=device)\n",
    "\n",
    "    return x, p, q, p_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Potential and true neighbors. Is good, but maybe find a way to do on CUDA? \"\"\"\n",
    "def find_potential_neighbours(x, k=100, distance_upper_bound=np.inf):\n",
    "    tree = cKDTree(x)                                                                      # Making a tree structure to find nearest neighbors quick\n",
    "    d, idx = tree.query(x, k + 1, distance_upper_bound=distance_upper_bound, workers=-1)   # Quering the tree we just made for k nearest neighbors \n",
    "    return d[:, 1:], idx[:, 1:]                                                            # Returning distances and the indexes of the found k nearest neighbors\n",
    "                                                                                           # The columns taken out are self-referential. (nearest neighbor to each point is itself.)\n",
    "\n",
    "def find_true_neighbours(d, dx):                                                                  # Takes two arrays: d: distances to nearest neighbors of shape (n,k) (n: n_points, k:number of nearest neighbors)\n",
    "                                                                                                  # and dx: tensor containing vectors pointing from each particles potential neighbor to it \n",
    "    with torch.no_grad():                                                                         # We don't need to calculate gradients here, so they are 'thrown away'. Lessens memory consumption.\n",
    "        z_masks = []                                                                              # lst that will contain the masks (GET BACK TO THIS)\n",
    "        i0 = 0                                                                                    # We do the following calculation in batches as the tensors get quite big\n",
    "        batch_size = 250                   \n",
    "        i1 = batch_size\n",
    "        while True:\n",
    "            if i0 >= dx.shape[0]:\n",
    "                break\n",
    "\n",
    "            n_dis = torch.sum((dx[i0:i1, :, None, :] / 2 - dx[i0:i1, None, :, :]) ** 2, dim=3)    # finding distances from k to the midpoint between i and j squared for all potential neighbors\n",
    "            n_dis += 1000 * torch.eye(n_dis.shape[1], device=device)[None, :, :]                  # We add 1000 to the sites that correspond to subtracting the vector ij/2 with ij, so these don't fuck anything up.\n",
    "\n",
    "            z_mask = torch.sum(n_dis < (d[i0:i1, :, None] ** 2 / 4), dim=2) <= 0                  # If all the distances from k to ij/2 are bigger than half the distance from i to j we use the connection ij.\n",
    "            z_masks.append(z_mask)                                                                # Boolean mask that can be applied to the idx (from the tree query) to ascertain whether they are true neighbors\n",
    "\n",
    "            if i1 > dx.shape[0]:\n",
    "                break\n",
    "            i0 = i1\n",
    "            i1 += batch_size\n",
    "    z_mask = torch.cat(z_masks, dim=0)\n",
    "    return z_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Calculating the potential. Needs to be changed quite a bit to suit our purposes\n",
    "    The way the lambda's are implemented needs to change\"\"\"\n",
    "\n",
    "#inputs changed from: def potential(x, p, q, idx, d, lam, alpha, z_mask, dx,m ):\n",
    "def potential(dx, p, q, idx, d, p_mask, z_mask, l00, l01, l1, l2, l3):\n",
    "\n",
    "    # Calculate S\n",
    "    pi = p[:, None, :].expand(p.shape[0], idx.shape[1], 3)                              # We expand the ABP tensor in order to be able to do all cross products in 1 go\n",
    "    pj = p[idx]                                                                         # For each particle we get the ABP of its nearest neighbors\n",
    "    \n",
    "    qi = q[:, None, :].expand(q.shape[0], idx.shape[1], 3)                              # Expansion for later cross product\n",
    "    qj = q[idx]                                                                         # For each particle we the the PCP of its nearest neighbors\n",
    "    \n",
    "    \n",
    "    interaction_mask = p_mask[:,None].expand(p_mask.shape[0], idx.shape[1]) + p_mask[idx]              # We make a mask of all particle interactions. 0 = completely non polar interaction. 1 = polar - nonpolar interaction. 2 = completely polar interaction.\n",
    "    lam = torch.zeros(size=(interaction_mask.shape[0], interaction_mask.shape[1], 4), device=device)   # Initializing an empty array for our lambdas\n",
    "    lam[interaction_mask == 0] = torch.tensor([l00,0,0,0], device=device)                              # Setting lambdas for non polar interaction\n",
    "    lam[interaction_mask == 1] = torch.tensor([l01,0,0,0], device=device)                              # Setting lambdas for polar-nonpolar interaction\n",
    "    lam[interaction_mask == 2] = torch.tensor([0,l1,l2,l3], device=device)                             # Setting lambdas for polar interaction\n",
    "    lam.requires_grad = True                                                                           # We need these gradients in order to do backprob later.\n",
    "\n",
    "\n",
    "    S1 = torch.sum(torch.cross(pj, dx, dim=2) * torch.cross(pi, dx, dim=2), dim=2)      #Calculating S1 (The ABP-position part of S). Scalar for each particle-interaction. Meaning we get array of size (n, m) , m being the max number of nearest neighbors for a particle\n",
    "    S2 = torch.sum(torch.cross(pi, qi, dim=2) * torch.cross(pj, qj, dim=2), dim=2)      #Calculating S2 (The ABP-PCP part of S).\n",
    "    S3 = torch.sum(torch.cross(qi, dx, dim=2) * torch.cross(qj, dx, dim=2), dim=2)      #Calculating S3 (The PCP-position part of S)\n",
    "\n",
    "    S = lam[:,:,0] + lam[:,:,1] * S1 + lam[:,:,2] * S2 + lam[:,:,3] * S3                #Calculating S total. Weighing each interaction by their appropriate lambas. array shape (n,m)\n",
    "\n",
    "    # Potential\n",
    "    Vij = z_mask.float() * (torch.exp(-d) - S * torch.exp(-d/5))                        #Calculating the potentials between all (nearest neighbor) particles\n",
    "    V = torch.sum(Vij)                                                                  #Calculating full potential for downstream backpropagation\n",
    "\n",
    "    return V                                                                            #We return full potential and max number of nearest neighbors for a particle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Actual simulation. Needs to change\"\"\"\n",
    "class TimeStepper:\n",
    "    def __init__(self, init_k):                                                   # Initialization stuff. Basically saving values for later.\n",
    "        self.k = init_k\n",
    "        self.true_neighbour_max = init_k//2\n",
    "        self.d = None\n",
    "        self.idx = None\n",
    "\n",
    "    def update_k(self, true_neighbour_max, tstep):                                # Function for changing k (number of neighbors found in tree) organically so we don't waste ressources\n",
    "        k = self.k\n",
    "        fraction = true_neighbour_max / k\n",
    "        if fraction < 0.25:                                                       # If fraction is small our k is too large and we make k smaller\n",
    "            k = int(0.75 * k)\n",
    "        elif fraction > 0.75:                                                     # Vice versa\n",
    "            k = int(1.5 * k)\n",
    "        n_update = 1 if tstep < 50 else max([1, int(20 * np.tanh(tstep / 200))])  # We don't find new neighbors for the first 50 timesteps. Afterwards we do when the tanh function 'allows'. Meaning we update frequently just after tstep=50 and then less and less frequent until we only update every 20. timestep\n",
    "        self.k = k                                                                # We update k\n",
    "        return k, n_update\n",
    "\n",
    "    def time_step(self, x, p, q, p_mask, tstep, dt, sqrt_dt, eta, l00, l01, l1, l2, l3):\n",
    "        # Idea: only update _potential_ neighbours every x steps late in simulation\n",
    "        # For now we do this on CPU, so transfer will be expensive\n",
    "\n",
    "        # Making assertions to make sure we ain't fucking something up\n",
    "        assert l1 + l2 + l3 == 1                                                  # Checking 'normalization' requirement of the lambas\n",
    "        assert q.shape == x.shape                                                 # Checking whether the shapes of our positions and polarizations are as they should be\n",
    "        assert x.shape == p.shape\n",
    "\n",
    "        # Finding potential neighbors at specific timesteps\n",
    "        k, n_update = self.update_k(self.true_neighbour_max, tstep)               # We update k and find n_update\n",
    "        if tstep % n_update == 0 or self.idx is None:                             #if n_update is the 'right value' we find potential neighbors again (maybe get back to thus) \n",
    "            d, idx = find_potential_neighbours(x.detach().to(\"cpu\").numpy(), k=k) #Find potential neighbors and their distances to the the queried particle\n",
    "            self.idx = torch.tensor(idx, dtype=torch.long, device=device)         #Update indices of potential neighbors\n",
    "            self.d = torch.tensor(d, dtype=torch.float, device=device)            #Update distances of potential neighbors\n",
    "        idx = self.idx\n",
    "        d = self.d   #IS THIS NECESSARY? I THINK WE OVERWRITE LATER / DONT USE\n",
    "\n",
    "        # Normalise p, q\n",
    "        with torch.no_grad():\n",
    "            p[p_mask != 0] /= torch.sqrt(torch.sum(p[p_mask != 0] ** 2, dim=1))[:, None]   # normalizing p. Only the non-zero polarities are considered.\n",
    "            q[p_mask != 0] /= torch.sqrt(torch.sum(q[p_mask != 0] ** 2, dim=1))[:, None]   # normalizing q. Only the non-zero polarities are considered.\n",
    "            p[p_mask == 0] = torch.tensor([0.,0.,0.], device=device)                       # make sure that our non-polarized particles stay non-polarized. I don't think this is strictly necessary, but it's nice to be sure.\n",
    "            q[p_mask == 0] = torch.tensor([0.,0.,0.], device=device)  \n",
    "\n",
    "        \n",
    "        # Find true neighbours\n",
    "        full_n_list = x[idx]                                                      # tensor containing the coordinates for all the potential neighbors of each particle\n",
    "        dx = x[:, None, :] - full_n_list                                          # tensor containing vectors pointing from each particles potential neighbor to it \n",
    "        z_mask = find_true_neighbours(d, dx)                                      # finding boolean mask that can be used on idx to find true \n",
    "        z_mask = z_mask.int()\n",
    "        \n",
    "        # Minimize size of z_mask and reorder idx and dx\n",
    "        sort_idx = torch.argsort(z_mask, dim=1, descending=True)                  # We find sort-indices that put all the 1's in our mask before all the 0's\n",
    "        z_mask = torch.gather(z_mask, 1, sort_idx)                                # We use the sorted indices we just found on the mask\n",
    "        dx = torch.gather(dx, 1, sort_idx[:, :, None].expand(-1, -1, 3))          # We use the sorted indices on the dx tensor (some broadcasting necessary)\n",
    "        idx = torch.gather(idx, 1, sort_idx)                                      # We use the sorted indices we just found on the potential neighbor indices\n",
    "        m = torch.max(torch.sum(z_mask, dim=1)) + 1                               # We sum along each of the possible neighbor bool masks and note the biggest one. This value is the maximal number of nearest neighbors that any particle has\n",
    "        z_mask = z_mask[:, :m]                                                    # We throw away as many zero's as we can. Corresponding to throwing away potential neighbors of particles that are not used due to voronoi conditions.\n",
    "        dx = dx[:, :m]                                                            # Same as above\n",
    "        idx = idx[:, :m]                                                          # Same as above\n",
    "        self.true_neighbour_max = m                                               # We update the maximal number of true neighbors\n",
    "        \n",
    "        # Normalize dx\n",
    "        d = torch.sqrt(torch.sum(dx**2, dim=2))                                   # We redefine d and normalize the dx's. We redefine so these new d's correspond with the sorting we have done just above.\n",
    "        dx = dx / d[:, :, None]\n",
    "        \n",
    "        # Calculate potential\n",
    "        V = potential(dx=dx, p=p, q=q, idx=idx, d=d, p_mask=p_mask, z_mask=z_mask,\n",
    "                       l00=l00, l01=l01, l1=l1, l2=l2, l3=l3)                     # We get the total potential (float)\n",
    "\n",
    "        # Backpropagation\n",
    "        V.backward()                                                              # Backpropagation. By the chain rule, all gradients are found and stored in their respective tensors \n",
    "\n",
    "        # Time-step\n",
    "        with torch.no_grad():                                                     # We use nograds in order not to fuck with the computational graph (I think)\n",
    "            x += -x.grad * dt + eta * float_tensor(*x.shape).normal_() * sqrt_dt  # Update our system by overdamped dynamics\n",
    "            p += -p.grad * dt + eta * float_tensor(*x.shape).normal_() * sqrt_dt  # Same as above\n",
    "            # q is kept fixed\n",
    "            #q += -q.grad * dt + eta * float_tensor(*x.shape).normal_() * sqrt_dt # We don't change our PCP (MAYBE LATER???)\n",
    "\n",
    "        # Zero gradients\n",
    "        x.grad.zero_()                                                            # Zero out the gradient so we can start again\n",
    "        p.grad.zero_()\n",
    "        q.grad.zero_()\n",
    "\n",
    "        # return x, p, q\n",
    "    \n",
    "def simulation(x, p, q, p_mask, dt, eta, l00, l01, l1, l2, l3, yield_every, init_k=200):\n",
    "    sqrt_dt = np.sqrt(dt)\n",
    "    time_stepper = TimeStepper(init_k=init_k)                                               # We initialize our timestepper class with the initial potential nearest neighbor number set to 200\n",
    "    tstep = 0                                                                               # Start at timestep 0  \n",
    "    while True:\n",
    "        tstep +=1                                                                           # Add one to our timestep\n",
    "        time_stepper.time_step(x=x, p=p, q=q, p_mask=p_mask, tstep=tstep, dt=dt, sqrt_dt=sqrt_dt,\n",
    "                                          eta=eta, l00=l00, l01=l01, l1=l1, l2=l2, l3=l3)   # We run one timestep of the simulation\n",
    "\n",
    "        if tstep % yield_every == 0:                                                        # Every yield_every timestep we save the positions, polarities and wedging coefficients for the system\n",
    "            xx = x.detach().to(\"cpu\").numpy()                                               # We detach our data from the GPU in order to save it\n",
    "            pp = p.detach().to(\"cpu\").numpy()\n",
    "            qq = q.detach().to(\"cpu\").numpy()\n",
    "            yield xx, pp, qq                                                                # Yield is basically a return statement that does not terminate the function but lets it run on\n",
    "\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(data_str, output_folder,  dt, eta, yield_steps, yield_every, l00, l01, l1, l2, l3):\n",
    "    x, p, q, p_mask = prepare_data(data_str)\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(output_folder)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    i = 0\n",
    "    scipy.io.savemat( output_folder + f'/t{i*yield_every}.mat', dict(x=x.detach().to(\"cpu\").numpy(),p=p.detach().to(\"cpu\").numpy(),q=q.detach().to(\"cpu\").numpy()))\n",
    "    np.save(output_folder + '/p_mask', p_mask.detach().to(\"cpu\").numpy())\n",
    "    for xx, pp, qq in itertools.islice(simulation(x=x, p=p, q=q, p_mask=p_mask, dt=dt, eta=eta, l00=l00,\n",
    "                                                    l01=l01, l1=l1, l2=l2, l3=l3, yield_every=yield_every), yield_steps):\n",
    "        i += 1\n",
    "        print(f'Running {i*yield_every} of {yield_steps*yield_every} timesteps', end='\\r')\n",
    "        scipy.io.savemat( output_folder + f'/t{i*yield_every}.mat', dict(x=xx,p=pp,q=qq))\n",
    "    print(f'Simulation done, saved {yield_steps+1} datapoints')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation done, saved 6 datapoints\n"
     ]
    }
   ],
   "source": [
    "run_simulation('cube1000_50p.npy', 'nonreturn_test', dt=0.2, eta=0, yield_steps=5, yield_every=500,\n",
    "               l00=0.7, l01=0.3, l1=1.0, l2=0.0, l3=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
